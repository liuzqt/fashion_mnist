{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unpack the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T03:46:25.067757Z",
     "start_time": "2017-11-06T03:46:25.064517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load unpack_data.py\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "train_images_idx3_ubyte_file = './data/train-images-idx3-ubyte'\n",
    "train_labels_idx1_ubyte_file = './data/train-labels-idx1-ubyte'\n",
    "\n",
    "test_images_idx3_ubyte_file = './data/t10k-images-idx3-ubyte'\n",
    "test_labels_idx1_ubyte_file = './data/t10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "def decode_idx3_ubyte(idx3_ubyte_file):\n",
    "    with open(idx3_ubyte_file, 'rb') as f:\n",
    "        bin_data = f.read()\n",
    "\n",
    "    # parse header\n",
    "    offset = 0\n",
    "    fmt_header = '>iiii'\n",
    "    magic_number, num_images, num_rows, num_cols = struct.unpack_from(\n",
    "        fmt_header, bin_data, offset)\n",
    "    print('total images: %d, image size: %d*%d' % (\n",
    "        num_images, num_rows, num_cols))\n",
    "\n",
    "    # parse data\n",
    "    image_size = num_rows * num_cols\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>' + str(image_size) + 'B'\n",
    "    images = np.empty((num_images, num_rows, num_cols))\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('parsed %d' % (i + 1))\n",
    "        images[i] = np.array(\n",
    "            struct.unpack_from(fmt_image, bin_data, offset)).reshape(\n",
    "            (num_rows, num_cols))\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def decode_idx1_ubyte(idx1_ubyte_file):\n",
    "    with open(idx1_ubyte_file, 'rb') as f:\n",
    "        bin_data = f.read()\n",
    "\n",
    "    # parse header\n",
    "    offset = 0\n",
    "    fmt_header = '>ii'\n",
    "    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('labels number: %d' % (num_images))\n",
    "\n",
    "    # parse data\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>B'\n",
    "    labels = np.empty(num_images)\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(\n",
    "                'parsed %d' % (i + 1))\n",
    "        labels[i] = struct.unpack_from(fmt_image, bin_data, offset)[0]\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return labels.astype(np.uint8)\n",
    "\n",
    "\n",
    "def unpack_data():\n",
    "    train_images = decode_idx3_ubyte(train_images_idx3_ubyte_file)\n",
    "    train_labels = decode_idx1_ubyte(train_labels_idx1_ubyte_file)\n",
    "    test_images = decode_idx3_ubyte(test_images_idx3_ubyte_file)\n",
    "    test_labels = decode_idx1_ubyte(test_labels_idx1_ubyte_file)\n",
    "\n",
    "    perm = np.arange(0, len(train_images))\n",
    "    np.random.shuffle(perm)\n",
    "    train_images = train_images[perm]\n",
    "    train_labels = train_labels[perm]\n",
    "\n",
    "    np.save('./data/train_data.npy', train_images)\n",
    "    np.save('./data/train_label.npy', train_labels)\n",
    "    np.save('./data/test_data.npy', test_images)\n",
    "    np.save('./data/test_label.npy', test_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unpack_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run a random forest for a quick baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T03:50:20.187288Z",
     "start_time": "2017-11-06T03:50:20.178199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load random_forest.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "train_data = np.load('./data/train_data.npy')\n",
    "train_data = np.reshape(train_data, [train_data.shape[0], -1])\n",
    "train_label = np.load('./data/train_label.npy')\n",
    "test_data = np.load('./data/test_data.npy')\n",
    "test_data = np.reshape(test_data, [test_data.shape[0], -1])\n",
    "test_label = np.load('./data/test_label.npy')\n",
    "\n",
    "print('random forest baseline')\n",
    "rf = RandomForestClassifier(n_estimators=70, n_jobs=-1, bootstrap=True)\n",
    "rf.fit(train_data, train_label)\n",
    "print('training finished')\n",
    "accuracy = rf.score(test_data, test_label)\n",
    "print('test accuracy: %f' % accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try a basic LeNet5\n",
    "\n",
    "It's a basic LeNet5 model with slightly modified. It can reach a pretty good accuracy on original MNIST, so let's see how well it performs on fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T03:58:07.406131Z",
     "start_time": "2017-11-06T03:58:07.402652Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load models/LeNet5.py\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class LeNet5(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # collect layers to calculate MI\n",
    "        self.layers_collector = []\n",
    "\n",
    "        if config.initializer == 'xavier':\n",
    "            self.initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        else:\n",
    "            self.initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "        if config.activate_func == 'sigmoid':\n",
    "            self.activate_func = tf.nn.sigmoid\n",
    "        elif config.activate_func == 'relu':\n",
    "            self.activate_func = tf.nn.relu\n",
    "        elif config.activate_func == 'tanh':\n",
    "            self.activate_func = tf.nn.tanh\n",
    "        else:\n",
    "            raise Exception('activation function not defined!')\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, 28, 28], name='input')\n",
    "        self._input = tf.expand_dims(self.input, 3)\n",
    "        self.label = tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "\n",
    "        # first conv+pooling\n",
    "        self.h1_conv = self.conv2d(self._input, 32, [5, 5], name='hidden1')\n",
    "        self.h1 = tf.layers.max_pooling2d(self.h1_conv, [2, 2], [2, 2],\n",
    "                                          name='pooling1')\n",
    "\n",
    "        # second conv+pooling\n",
    "        self.h2_conv = self.conv2d(self.h1, 64, [5, 5], name='hidden2')\n",
    "        self.h2 = tf.layers.max_pooling2d(self.h2_conv, [2, 2], [2, 2],\n",
    "                                          name='pooling2')\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = tf.reshape(self.h2, [-1, 7 * 7 * 64], 'flatten')\n",
    "\n",
    "        # fc1\n",
    "        self.fc1 = tf.layers.dense(self.flatten, 1024, self.activate_func,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc1')\n",
    "\n",
    "        # dropout\n",
    "        if config.dropout:\n",
    "            self.dropout = tf.nn.dropout(self.fc1, config.keep_prob)\n",
    "        else:\n",
    "            self.dropout = self.fc1\n",
    "\n",
    "        self.fc2 = tf.layers.dense(self.fc1, 10,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc2')\n",
    "        self.softmax = tf.nn.softmax(logits=self.fc2, name='softmax')\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(self.softmax, 1), tf.argmax(self.label, 1)),\n",
    "            tf.float32), name='accuracy')\n",
    "\n",
    "        # loss and gradient\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        initial_learning_rate = tf.Variable(\n",
    "            config.learning_rate, trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(\n",
    "            initial_learning_rate, self.global_step, self.config.decay_step,\n",
    "            self.config.lr_decay,\n",
    "            name='lr') if config.use_lr_decay else initial_learning_rate\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "        self.loss = -tf.reduce_sum(self.label * tf.log(self.softmax))\n",
    "        self.train_op = self.optimizer.minimize(self.loss,\n",
    "                                                global_step=self.global_step)\n",
    "\n",
    "        self.layers_collector.append(self.input)\n",
    "        self.layers_collector.append(self.transpose(self.h1))\n",
    "        self.layers_collector.append(self.transpose(self.h2))\n",
    "        self.layers_collector.append(tf.expand_dims(self.fc1, 1))\n",
    "        self.layers_collector.append(self.softmax)\n",
    "\n",
    "    def transpose(self, layer):\n",
    "        return tf.transpose(layer, [0, 3, 1, 2])\n",
    "\n",
    "    def conv2d(self, input, channel, kernel, name=None):\n",
    "        l2_regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            scale=self.config.l2_beta) if self.config.l2_norm else None\n",
    "\n",
    "        conv = tf.layers.conv2d(input, channel, kernel,\n",
    "                                strides=(1, 1), padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                kernel_initializer=self.initializer,\n",
    "                                kernel_regularizer=l2_regularizer)\n",
    "        if self.config.batch_norm:\n",
    "            conv = tf.layers.batch_normalization(conv)\n",
    "        activate = self.activate_func(conv, name)\n",
    "        return activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T03:52:39.771739Z",
     "start_time": "2017-11-06T03:52:39.768583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load runner.py\n",
    "from reader import read_dataset\n",
    "from models.LeNet5 import LeNet5\n",
    "from models.ResNet import ResNet\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "import signal\n",
    "from config import get_config\n",
    "from entropy import entropy\n",
    "from plot import plot_info_plain\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "class Runner(object):\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.dataset = read_dataset(config.batch_size, config.valid_size,\n",
    "                                    config.sample_size)\n",
    "        self.graph = tf.Graph()\n",
    "        self.model = None\n",
    "        self.restore = False\n",
    "        if not os.path.exists(self.config.model_path):\n",
    "            os.mkdir(self.config.model_path)\n",
    "        for key in config.__dict__:\n",
    "            print(key, config.__dict__[key])\n",
    "        with self.graph.as_default():\n",
    "            self.model = model(self.config)\n",
    "\n",
    "        self.IXT = []\n",
    "        self.ITY = []\n",
    "\n",
    "    def run(self):\n",
    "        with self.graph.as_default(), tf.Session() as sess:\n",
    "            self.restore = True\n",
    "            model_path = os.path.join(self.config.model_path,\n",
    "                                      self.config.model_name)\n",
    "            saver = tf.train.Saver()\n",
    "            files = glob(os.path.join(self.config.model_path, '*.ckpt.*'))\n",
    "\n",
    "            if len(files) > 0:\n",
    "                saver.restore(sess, model_path)\n",
    "                print(('Model restored from:' + model_path))\n",
    "            else:\n",
    "                print(\"Model doesn't exist.\\nInitializing........\")\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            def handler_stop_signals(signum, frame):\n",
    "\n",
    "                print(\n",
    "                    'training shut down,  the model will be save in %s' % (\n",
    "                        model_path))\n",
    "                saver.save(sess, save_path=model_path)\n",
    "                sys.exit(0)\n",
    "\n",
    "            signal.signal(signal.SIGINT, handler_stop_signals)\n",
    "            signal.signal(signal.SIGTERM, handler_stop_signals)\n",
    "\n",
    "            while self.dataset.epoch < self.config.max_epoch:\n",
    "                data, label = self.dataset.next_training_batch()\n",
    "                _, step, loss, layers = sess.run(\n",
    "                    [self.model.train_op, self.model.global_step,\n",
    "                     self.model.loss, self.model.layers_collector],\n",
    "                    feed_dict={self.model.input: data,\n",
    "                               self.model.label: label})\n",
    "\n",
    "                if step % self.config.valid_step == 0:\n",
    "                    valid_data, valid_label = self.dataset.valid_batch()\n",
    "                    accu = sess.run(self.model.accuracy,\n",
    "                                    feed_dict={self.model.input: valid_data,\n",
    "                                               self.model.label: valid_label})\n",
    "                    print('step %d, epoch %d, valid accuracy: %f' % (\n",
    "                        step, self.dataset.epoch, accu))\n",
    "                if step % self.config.info_plane_interval == 0:\n",
    "                    print('flag')\n",
    "                    sample_data = self.dataset.sample_batch()\n",
    "                    layers = sess.run(self.model.layers_collector,\n",
    "                                      feed_dict={self.model.input: sample_data\n",
    "                                                 })\n",
    "                    ixt, ity = entropy(layers)\n",
    "\n",
    "                    self.IXT.append(ixt)\n",
    "                    self.ITY.append(ity)\n",
    "\n",
    "            self._test(sess)\n",
    "            saver.save(sess, save_path=model_path)\n",
    "            print(\n",
    "                'training finished,  the model will be save in %s' % (\n",
    "                    self.config.model_path))\n",
    "\n",
    "    def test(self):\n",
    "        with self.graph.as_default(), tf.Session() as sess:\n",
    "            files = glob(os.path.join(self.config.model_path, '*.ckpt.*'))\n",
    "            assert len(files) > 0\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, os.path.join(self.config.model_path,\n",
    "                                             self.config.model_name))\n",
    "            print(('Model restored from:' + self.config.model_path))\n",
    "            self._test(sess)\n",
    "            self.plot_info_plane()\n",
    "\n",
    "    def plot_info_plane(self):\n",
    "        with open('ixt', 'wb') as f:\n",
    "            pickle.dump(self.IXT, f)\n",
    "        with open('ity', 'wb') as f:\n",
    "            pickle.dump(self.ITY, f)\n",
    "        plot_info_plain(self.IXT, self.ITY)\n",
    "\n",
    "    def _test(self, sess):\n",
    "        test_data, test_label = self.dataset.test_batch()\n",
    "        accu = sess.run(self.model.accuracy,\n",
    "                        feed_dict={self.model.input: test_data,\n",
    "                                   self.model.label: test_label})\n",
    "        print('test accuracy:%f' % accu)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    runner = Runner(get_config(), LeNet5)\n",
    "    runner.run()\n",
    "    # runner.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try ResNet\n",
    "based on the idea of resnet, I build a little bit deeper network, with 6 CNN layers and 2 fc layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T04:00:51.913538Z",
     "start_time": "2017-11-06T04:00:51.910151Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load models/ResNet.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResNet(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # collect layers to calculate MI\n",
    "        self.layers_collector = []\n",
    "\n",
    "        if config.initializer == 'xavier':\n",
    "            self.initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        else:\n",
    "            self.initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "        if config.activate_func == 'sigmoid':\n",
    "            self.activate_func = tf.nn.sigmoid\n",
    "        elif config.activate_func == 'relu':\n",
    "            self.activate_func = tf.nn.relu\n",
    "        elif config.activate_func == 'tanh':\n",
    "            self.activate_func = tf.nn.tanh\n",
    "        else:\n",
    "            raise Exception('activation function not defined!')\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, 28, 28], name='input')\n",
    "        self._input = tf.expand_dims(self.input, 3)\n",
    "        self.label = tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "        self.layers_collector.append(self.input)\n",
    "\n",
    "        # first res block\n",
    "        self.block1 = self.res_block(self._input, 32)\n",
    "        self.pooling1 = tf.layers.max_pooling2d(self.block1, [2, 2], [2, 2],\n",
    "                                                name='pooling1')\n",
    "\n",
    "        # second res block\n",
    "        self.block2 = self.res_block(self.block1, 64)\n",
    "        self.pooling2 = tf.layers.max_pooling2d(self.block2, [2, 2], [2, 2],\n",
    "                                                name='pooling2')\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = tf.reshape(self.pooling2, [-1, 7 * 7 * 64], 'flatten')\n",
    "\n",
    "        # fc1\n",
    "        self.fc1 = tf.layers.dense(self.flatten, 1024, self.activate_func,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc1')\n",
    "\n",
    "        # dropout\n",
    "        if config.dropout:\n",
    "            self.dropout = tf.nn.dropout(self.fc1, config.keep_prob)\n",
    "        else:\n",
    "            self.dropout = self.fc1\n",
    "\n",
    "        self.fc2 = tf.layers.dense(self.fc1, 10,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc2')\n",
    "        self.softmax = tf.nn.softmax(logits=self.fc2, name='softmax')\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(self.softmax, 1), tf.argmax(self.label, 1)),\n",
    "            tf.float32), name='accuracy')\n",
    "\n",
    "        # loss and gradient\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        initial_learning_rate = tf.Variable(\n",
    "            config.learning_rate, trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(\n",
    "            initial_learning_rate, self.global_step, self.config.decay_step,\n",
    "            self.config.lr_decay,\n",
    "            name='lr') if config.use_lr_decay else initial_learning_rate\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "        self.loss = -tf.reduce_sum(self.label * tf.log(self.softmax))\n",
    "        self.train_op = self.optimizer.minimize(self.loss,\n",
    "                                                global_step=self.global_step)\n",
    "\n",
    "        self.layers_collector.append(tf.expand_dims(self.fc1, 1))\n",
    "        self.layers_collector.append(self.softmax)\n",
    "\n",
    "    def res_block(self, x, channel):\n",
    "        for i in range(3):\n",
    "            shortcut = x\n",
    "            if i == 1:\n",
    "                shortcut = self.conv2d(shortcut, channel, [1, 1])\n",
    "\n",
    "            x = self.conv2d(x, channel, [3, 3])\n",
    "            x += shortcut\n",
    "            self.layers_collector.append(x)\n",
    "        return x\n",
    "\n",
    "    def transpose(self, layer):\n",
    "        return tf.transpose(layer, [0, 3, 1, 2])\n",
    "\n",
    "    def conv2d(self, input, channel, kernel, name=None):\n",
    "        l2_regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            scale=self.config.l2_beta) if self.config.l2_norm else None\n",
    "\n",
    "        conv = tf.layers.conv2d(input, channel, kernel,\n",
    "                                strides=(1, 1), padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                kernel_initializer=self.initializer,\n",
    "                                kernel_regularizer=l2_regularizer)\n",
    "        if self.config.batch_norm:\n",
    "            conv = tf.layers.batch_normalization(conv)\n",
    "        activate = self.activate_func(conv, name)\n",
    "        return activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runner = Runner(get_config(),ResNet)\n",
    "runner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
