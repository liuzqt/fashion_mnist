{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unpack the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T03:46:25.067757Z",
     "start_time": "2017-11-06T03:46:25.064517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images: 60000, image size: 28*28\n",
      "parsed 10000\n",
      "parsed 20000\n",
      "parsed 30000\n",
      "parsed 40000\n",
      "parsed 50000\n",
      "parsed 60000\n",
      "labels number: 60000\n",
      "parsed 10000\n",
      "parsed 20000\n",
      "parsed 30000\n",
      "parsed 40000\n",
      "parsed 50000\n",
      "parsed 60000\n",
      "total images: 10000, image size: 28*28\n",
      "parsed 10000\n",
      "labels number: 10000\n",
      "parsed 10000\n"
     ]
    }
   ],
   "source": [
    "# %load unpack_data.py\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "train_images_idx3_ubyte_file = './data/train-images-idx3-ubyte'\n",
    "train_labels_idx1_ubyte_file = './data/train-labels-idx1-ubyte'\n",
    "\n",
    "test_images_idx3_ubyte_file = './data/t10k-images-idx3-ubyte'\n",
    "test_labels_idx1_ubyte_file = './data/t10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "def decode_idx3_ubyte(idx3_ubyte_file):\n",
    "    with open(idx3_ubyte_file, 'rb') as f:\n",
    "        bin_data = f.read()\n",
    "\n",
    "    # parse header\n",
    "    offset = 0\n",
    "    fmt_header = '>iiii'\n",
    "    magic_number, num_images, num_rows, num_cols = struct.unpack_from(\n",
    "        fmt_header, bin_data, offset)\n",
    "    print('total images: %d, image size: %d*%d' % (\n",
    "        num_images, num_rows, num_cols))\n",
    "\n",
    "    # parse data\n",
    "    image_size = num_rows * num_cols\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>' + str(image_size) + 'B'\n",
    "    images = np.empty((num_images, num_rows, num_cols))\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('parsed %d' % (i + 1))\n",
    "        images[i] = np.array(\n",
    "            struct.unpack_from(fmt_image, bin_data, offset)).reshape(\n",
    "            (num_rows, num_cols))\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def decode_idx1_ubyte(idx1_ubyte_file):\n",
    "    with open(idx1_ubyte_file, 'rb') as f:\n",
    "        bin_data = f.read()\n",
    "\n",
    "    # parse header\n",
    "    offset = 0\n",
    "    fmt_header = '>ii'\n",
    "    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('labels number: %d' % (num_images))\n",
    "\n",
    "    # parse data\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>B'\n",
    "    labels = np.empty(num_images)\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(\n",
    "                'parsed %d' % (i + 1))\n",
    "        labels[i] = struct.unpack_from(fmt_image, bin_data, offset)[0]\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return labels.astype(np.uint8)\n",
    "\n",
    "\n",
    "def unpack_data():\n",
    "    train_images = decode_idx3_ubyte(train_images_idx3_ubyte_file)\n",
    "    train_labels = decode_idx1_ubyte(train_labels_idx1_ubyte_file)\n",
    "    test_images = decode_idx3_ubyte(test_images_idx3_ubyte_file)\n",
    "    test_labels = decode_idx1_ubyte(test_labels_idx1_ubyte_file)\n",
    "\n",
    "    perm = np.arange(0, len(train_images))\n",
    "    np.random.shuffle(perm)\n",
    "    train_images = train_images[perm]\n",
    "    train_labels = train_labels[perm]\n",
    "\n",
    "    np.save('./data/train_data.npy', train_images)\n",
    "    np.save('./data/train_label.npy', train_labels)\n",
    "    np.save('./data/test_data.npy', test_images)\n",
    "    np.save('./data/test_label.npy', test_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unpack_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run a random forest for a quick baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T03:50:20.187288Z",
     "start_time": "2017-11-06T03:50:20.178199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest baseline\n",
      "training finished\n",
      "test accuracy: 0.873500\n"
     ]
    }
   ],
   "source": [
    "# %load random_forest.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "train_data = np.load('./data/train_data.npy')\n",
    "train_data = np.reshape(train_data, [train_data.shape[0], -1])\n",
    "train_label = np.load('./data/train_label.npy')\n",
    "test_data = np.load('./data/test_data.npy')\n",
    "test_data = np.reshape(test_data, [test_data.shape[0], -1])\n",
    "test_label = np.load('./data/test_label.npy')\n",
    "\n",
    "print('random forest baseline')\n",
    "rf = RandomForestClassifier(n_estimators=70, n_jobs=-1, bootstrap=True)\n",
    "rf.fit(train_data, train_label)\n",
    "print('training finished')\n",
    "accuracy = rf.score(test_data, test_label)\n",
    "print('test accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try a basic LeNet5\n",
    "\n",
    "It's a basic LeNet5 model with slightly modified. It can reach a pretty good accuracy on original MNIST, so let's see how well it performs on fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T18:20:12.010645Z",
     "start_time": "2017-11-06T18:20:12.006699Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load models/LeNet5.py\n",
    "\n",
    "'''\n",
    "\n",
    "@author: ZiqiLiu\n",
    "\n",
    "\n",
    "@file: LeNet5.py\n",
    "\n",
    "@time: 2017/11/3 下午10:39\n",
    "\n",
    "@desc:\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class LeNet5(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # collect layers to calculate MI\n",
    "        self.layers_collector = []\n",
    "\n",
    "        if config.initializer == 'xavier':\n",
    "            self.initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        else:\n",
    "            self.initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "        if config.activate_func == 'sigmoid':\n",
    "            self.activate_func = tf.nn.sigmoid\n",
    "        elif config.activate_func == 'relu':\n",
    "            self.activate_func = tf.nn.relu\n",
    "        elif config.activate_func == 'tanh':\n",
    "            self.activate_func = tf.nn.tanh\n",
    "        else:\n",
    "            raise Exception('activation function not defined!')\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, 28, 28], name='input')\n",
    "        self._input = tf.expand_dims(self.input, 3)\n",
    "        self.label = tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "\n",
    "        # first conv+pooling\n",
    "        self.h1_conv = self.conv2d(self._input, 32, [5, 5], name='hidden1')\n",
    "        self.h1 = tf.layers.max_pooling2d(self.h1_conv, [2, 2], [2, 2],\n",
    "                                          name='pooling1')\n",
    "\n",
    "        # second conv+pooling\n",
    "        self.h2_conv = self.conv2d(self.h1, 64, [5, 5], name='hidden2')\n",
    "        self.h2 = tf.layers.max_pooling2d(self.h2_conv, [2, 2], [2, 2],\n",
    "                                          name='pooling2')\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = tf.reshape(self.h2, [-1, 7 * 7 * 64], 'flatten')\n",
    "\n",
    "        # fc1\n",
    "        self.fc1 = tf.layers.dense(self.flatten, 1024, self.activate_func,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc1')\n",
    "\n",
    "        # dropout\n",
    "        if config.dropout:\n",
    "            self.dropout = tf.nn.dropout(self.fc1, config.keep_prob)\n",
    "        else:\n",
    "            self.dropout = self.fc1\n",
    "\n",
    "        self.fc2 = tf.layers.dense(self.fc1, 10,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc2')\n",
    "        self.softmax = tf.nn.softmax(logits=self.fc2, name='softmax')\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(self.softmax, 1), tf.argmax(self.label, 1)),\n",
    "            tf.float32), name='accuracy')\n",
    "\n",
    "        # loss and gradient\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        initial_learning_rate = tf.Variable(\n",
    "            config.learning_rate, trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(\n",
    "            initial_learning_rate, self.global_step, self.config.decay_step,\n",
    "            self.config.lr_decay,\n",
    "            name='lr') if config.use_lr_decay else initial_learning_rate\n",
    "        if self.config.optimizer == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        else:\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(\n",
    "                self.learning_rate)\n",
    "\n",
    "        self.loss = -tf.reduce_sum(self.label * tf.log(self.softmax))\n",
    "        self.train_op = self.optimizer.minimize(self.loss,\n",
    "                                                global_step=self.global_step)\n",
    "\n",
    "        self.layers_collector.append(self.input)\n",
    "        self.layers_collector.append(self.transpose(self.h1))\n",
    "        self.layers_collector.append(self.transpose(self.h2))\n",
    "        self.layers_collector.append(tf.expand_dims(self.fc1, 1))\n",
    "        self.layers_collector.append(self.softmax)\n",
    "\n",
    "    def transpose(self, layer):\n",
    "        return tf.transpose(layer, [0, 3, 1, 2])\n",
    "\n",
    "    def conv2d(self, input, channel, kernel, name=None):\n",
    "        l2_regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            scale=self.config.l2_beta) if self.config.l2_norm else None\n",
    "\n",
    "        conv = tf.layers.conv2d(input, channel, kernel,\n",
    "                                strides=(1, 1), padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                kernel_initializer=self.initializer,\n",
    "                                kernel_regularizer=l2_regularizer)\n",
    "        if self.config.batch_norm:\n",
    "            conv = tf.layers.batch_normalization(conv)\n",
    "        activate = self.activate_func(conv, name)\n",
    "        return activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T18:20:23.602197Z",
     "start_time": "2017-11-06T18:20:23.598630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_lr_decay True\n",
      "batch_norm False\n",
      "l2_norm True\n",
      "model_path ./trained_model/\n",
      "decay_step 1200\n",
      "learning_rate 0.0005\n",
      "optimizer adam\n",
      "initializer xavier\n",
      "valid_step 1200\n",
      "valid_size 1000\n",
      "info_plane_interval 1200\n",
      "dropout True\n",
      "keep_prob 0.6\n",
      "max_epoch 20\n",
      "sample_size 2000\n",
      "model_name latest.ckpt\n",
      "lr_decay 0.85\n",
      "l2_beta 0.01\n",
      "batch_size 50\n",
      "activate_func tanh\n",
      "Model doesn't exist.\n",
      "Initializing........\n",
      "step 1200, epoch 1, loss 19.575270, valid accuracy: 0.889000\n",
      "step 2400, epoch 2, loss 12.733609, valid accuracy: 0.892000\n",
      "step 3600, epoch 3, loss 10.252765, valid accuracy: 0.919000\n",
      "step 4800, epoch 4, loss 8.187800, valid accuracy: 0.924000\n",
      "step 6000, epoch 5, loss 6.482375, valid accuracy: 0.913000\n",
      "step 7200, epoch 6, loss 4.781872, valid accuracy: 0.928000\n",
      "step 8400, epoch 7, loss 3.553936, valid accuracy: 0.931000\n",
      "step 9600, epoch 8, loss 2.397165, valid accuracy: 0.933000\n",
      "step 10800, epoch 9, loss 1.493087, valid accuracy: 0.935000\n",
      "step 12000, epoch 10, loss 0.973497, valid accuracy: 0.936000\n",
      "step 13200, epoch 11, loss 0.569833, valid accuracy: 0.936000\n",
      "step 14400, epoch 12, loss 0.377874, valid accuracy: 0.937000\n",
      "step 15600, epoch 13, loss 0.201938, valid accuracy: 0.933000\n",
      "step 16800, epoch 14, loss 0.126196, valid accuracy: 0.937000\n",
      "step 18000, epoch 15, loss 0.080657, valid accuracy: 0.935000\n",
      "step 19200, epoch 16, loss 0.058649, valid accuracy: 0.932000\n",
      "step 20400, epoch 17, loss 0.040313, valid accuracy: 0.930000\n",
      "step 21600, epoch 18, loss 0.028683, valid accuracy: 0.930000\n",
      "step 22800, epoch 19, loss 0.020440, valid accuracy: 0.938000\n",
      "test accuracy:0.921100\n",
      "training finished,  the model will be save in ./trained_model/\n"
     ]
    }
   ],
   "source": [
    "# %load runner.py\n",
    "\n",
    "'''\n",
    "\n",
    "@author: ZiqiLiu\n",
    "\n",
    "\n",
    "@file: runner.py\n",
    "\n",
    "@time: 2017/11/3 下午11:34\n",
    "\n",
    "@desc:\n",
    "'''\n",
    "from reader import read_dataset\n",
    "from models.LeNet5 import LeNet5\n",
    "from models.ResNet import ResNet\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "import signal\n",
    "from config import get_config\n",
    "from entropy import entropy\n",
    "from plot import plot_info_plain\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Runner(object):\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.dataset = read_dataset(config.batch_size, config.valid_size,\n",
    "                                    config.sample_size)\n",
    "        self.graph = tf.Graph()\n",
    "        self.model = None\n",
    "        self.restore = False\n",
    "        if not os.path.exists(self.config.model_path):\n",
    "            os.mkdir(self.config.model_path)\n",
    "        for key in config.__dict__:\n",
    "            print(key, config.__dict__[key])\n",
    "        with self.graph.as_default():\n",
    "            self.model = model(self.config)\n",
    "\n",
    "        self.IXT = []\n",
    "        self.ITY = []\n",
    "\n",
    "    def run(self):\n",
    "        with self.graph.as_default(), tf.Session() as sess:\n",
    "            self.restore = True\n",
    "            model_path = os.path.join(self.config.model_path,\n",
    "                                      self.config.model_name)\n",
    "            saver = tf.train.Saver()\n",
    "            files = glob(os.path.join(self.config.model_path, '*.ckpt.*'))\n",
    "\n",
    "            if len(files) > 0:\n",
    "                saver.restore(sess, model_path)\n",
    "                print(('Model restored from:' + model_path))\n",
    "            else:\n",
    "                print(\"Model doesn't exist.\\nInitializing........\")\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            def handler_stop_signals(signum, frame):\n",
    "\n",
    "                print(\n",
    "                    'training shut down,  the model will be save in %s' % (\n",
    "                        model_path))\n",
    "                saver.save(sess, save_path=model_path)\n",
    "                sys.exit(0)\n",
    "\n",
    "            signal.signal(signal.SIGINT, handler_stop_signals)\n",
    "            signal.signal(signal.SIGTERM, handler_stop_signals)\n",
    "\n",
    "            total_loss = 0\n",
    "            while self.dataset.epoch < self.config.max_epoch:\n",
    "                data, label = self.dataset.next_training_batch()\n",
    "                _, step, loss, layers = sess.run(\n",
    "                    [self.model.train_op, self.model.global_step,\n",
    "                     self.model.loss, self.model.layers_collector],\n",
    "                    feed_dict={self.model.input: data,\n",
    "                               self.model.label: label})\n",
    "                total_loss += loss\n",
    "                # ixt, ity = entropy(layers)\n",
    "                # self.IXT.append(ixt)\n",
    "                # self.ITY.append(ity)\n",
    "                if step % self.config.valid_step == 0:\n",
    "                    valid_data, valid_label = self.dataset.valid_batch()\n",
    "                    accu = sess.run(self.model.accuracy,\n",
    "                                    feed_dict={self.model.input: valid_data,\n",
    "                                               self.model.label: valid_label})\n",
    "                    print('step %d, epoch %d, loss %f, valid accuracy: %f' % (\n",
    "                        step, self.dataset.epoch,\n",
    "                        total_loss / self.config.valid_step, accu))\n",
    "                    total_loss = 0\n",
    "                if step % self.config.info_plane_interval == 0:\n",
    "                    sample_data = self.dataset.sample_batch()\n",
    "                    layers = sess.run(self.model.layers_collector,\n",
    "                                      feed_dict={self.model.input: sample_data\n",
    "                                                 })\n",
    "                    ixt, ity = entropy(layers)\n",
    "                    # print(ixt)\n",
    "                    # print(ity)\n",
    "                    with open('layers.pkl', 'wb') as f:\n",
    "                        pickle.dump(layers, f)\n",
    "\n",
    "                    self.IXT.append(ixt)\n",
    "                    self.ITY.append(ity)\n",
    "\n",
    "            self._test(sess)\n",
    "            saver.save(sess, save_path=model_path)\n",
    "            print(\n",
    "                'training finished,  the model will be save in %s' % (\n",
    "                    self.config.model_path))\n",
    "\n",
    "    def test(self):\n",
    "        with self.graph.as_default(), tf.Session() as sess:\n",
    "            files = glob(os.path.join(self.config.model_path, '*.ckpt.*'))\n",
    "            assert len(files) > 0\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, os.path.join(self.config.model_path,\n",
    "                                             self.config.model_name))\n",
    "            print(('Model restored from:' + self.config.model_path))\n",
    "            self._test(sess)\n",
    "            self.plot_info_plane()\n",
    "\n",
    "    def plot_info_plane(self):\n",
    "        with open('ixt', 'wb') as f:\n",
    "            pickle.dump(self.IXT, f)\n",
    "        with open('ity', 'wb') as f:\n",
    "            pickle.dump(self.ITY, f)\n",
    "        plot_info_plain(self.IXT, self.ITY)\n",
    "\n",
    "    def _test(self, sess):\n",
    "        test_data, test_label = self.dataset.test_batch()\n",
    "        accu = sess.run(self.model.accuracy,\n",
    "                        feed_dict={self.model.input: test_data,\n",
    "                                   self.model.label: test_label})\n",
    "        print('test accuracy:%f' % accu)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model = LeNet5\n",
    "    if len(sys.argv) > 1:\n",
    "        if sys.argv[1] == 'LeNet5':\n",
    "            print(\"Using LetNet5\")\n",
    "            model = LeNet5\n",
    "        elif sys.argv[1] == 'miniResNet':\n",
    "            print(\"Using miniResNet\")\n",
    "            model = ResNet\n",
    "    if model:\n",
    "        runner = Runner(get_config(), LeNet5)\n",
    "        runner.run()\n",
    "    else:\n",
    "        print('model not defined!')\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try ResNet\n",
    "based on the idea of resnet, I build a mini resnet, with 6 CNN layers and 2 fc layers. See if can achieve better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T18:20:33.741962Z",
     "start_time": "2017-11-06T18:20:33.738061Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load models/ResNet.py\n",
    "\n",
    "'''\n",
    "\n",
    "@author: ZiqiLiu\n",
    "\n",
    "\n",
    "@file: LeNet5.py\n",
    "\n",
    "@time: 2017/11/3 下午10:39\n",
    "\n",
    "@desc:\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResNet(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # collect layers to calculate MI\n",
    "        self.layers_collector = []\n",
    "\n",
    "        if config.initializer == 'xavier':\n",
    "            self.initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        else:\n",
    "            self.initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "        if config.activate_func == 'sigmoid':\n",
    "            self.activate_func = tf.nn.sigmoid\n",
    "        elif config.activate_func == 'relu':\n",
    "            self.activate_func = tf.nn.relu\n",
    "        elif config.activate_func == 'tanh':\n",
    "            self.activate_func = tf.nn.tanh\n",
    "        else:\n",
    "            raise Exception('activation function not defined!')\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, 28, 28], name='input')\n",
    "        self._input = tf.expand_dims(self.input, 3)\n",
    "        self.label = tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "        self.layers_collector.append(self.input)\n",
    "\n",
    "        # first res block\n",
    "        self.block1 = self.res_block(self._input, 32)\n",
    "        self.pooling1 = tf.layers.max_pooling2d(self.block1, [2, 2], [2, 2],\n",
    "                                                name='pooling1')\n",
    "\n",
    "        # second res block\n",
    "        self.block2 = self.res_block(self.pooling1, 64)\n",
    "        self.pooling2 = tf.layers.max_pooling2d(self.block2, [2, 2], [2, 2],\n",
    "                                                name='pooling2')\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = tf.reshape(self.pooling2, [-1, 7 * 7 * 64], 'flatten')\n",
    "\n",
    "        # fc1\n",
    "        self.fc1 = tf.layers.dense(self.flatten, 1024, self.activate_func,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc1')\n",
    "\n",
    "        # dropout\n",
    "        if config.dropout:\n",
    "            self.dropout = tf.nn.dropout(self.fc1, config.keep_prob)\n",
    "        else:\n",
    "            self.dropout = self.fc1\n",
    "\n",
    "        self.fc2 = tf.layers.dense(self.fc1, 10,\n",
    "                                   kernel_initializer=self.initializer,\n",
    "                                   name='fc2')\n",
    "        self.softmax = tf.nn.softmax(logits=self.fc2, name='softmax')\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(self.softmax, 1), tf.argmax(self.label, 1)),\n",
    "            tf.float32), name='accuracy')\n",
    "\n",
    "        # loss and gradient\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        initial_learning_rate = tf.Variable(\n",
    "            config.learning_rate, trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(\n",
    "            initial_learning_rate, self.global_step, self.config.decay_step,\n",
    "            self.config.lr_decay,\n",
    "            name='lr') if config.use_lr_decay else initial_learning_rate\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "        self.loss = -tf.reduce_sum(self.label * tf.log(self.softmax))\n",
    "        self.train_op = self.optimizer.minimize(self.loss,\n",
    "                                                global_step=self.global_step)\n",
    "\n",
    "        self.layers_collector.append(tf.expand_dims(self.fc1, 1))\n",
    "        self.layers_collector.append(self.softmax)\n",
    "\n",
    "    def res_block(self, x, channel):\n",
    "        for i in range(3):\n",
    "            shortcut = x\n",
    "            if i == 0:\n",
    "                shortcut = self.conv2d(shortcut, channel, [1, 1])\n",
    "\n",
    "            x = self.conv2d(x, channel, [3, 3])\n",
    "            x += shortcut\n",
    "            self.layers_collector.append(self.transpose(x))\n",
    "        return x\n",
    "\n",
    "    def transpose(self, layer):\n",
    "        return tf.transpose(layer, [0, 3, 1, 2])\n",
    "\n",
    "    def conv2d(self, input, channel, kernel, name=None):\n",
    "        l2_regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            scale=self.config.l2_beta) if self.config.l2_norm else None\n",
    "\n",
    "        conv = tf.layers.conv2d(input, channel, kernel,\n",
    "                                strides=(1, 1), padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                kernel_initializer=self.initializer,\n",
    "                                kernel_regularizer=l2_regularizer)\n",
    "        if self.config.batch_norm:\n",
    "            conv = tf.layers.batch_normalization(conv)\n",
    "        activate = self.activate_func(conv, name)\n",
    "        return activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using miniResNet\n",
      "use_lr_decay True\n",
      "batch_norm False\n",
      "l2_norm True\n",
      "model_path ./trained_model/\n",
      "decay_step 1200\n",
      "learning_rate 0.0005\n",
      "optimizer adam\n",
      "initializer xavier\n",
      "valid_step 1200\n",
      "valid_size 1000\n",
      "info_plane_interval 1200\n",
      "dropout True\n",
      "keep_prob 0.6\n",
      "max_epoch 20\n",
      "sample_size 2000\n",
      "model_name latest.ckpt\n",
      "lr_decay 0.85\n",
      "l2_beta 0.01\n",
      "batch_size 50\n",
      "activate_func tanh\n",
      "Model doesn't exist.\n",
      "Initializing........\n",
      "step 1200, epoch 1, loss 19.688631, valid accuracy: 0.880000\n",
      "step 2400, epoch 2, loss 12.870801, valid accuracy: 0.901000\n",
      "step 3600, epoch 3, loss 10.321647, valid accuracy: 0.912000\n",
      "step 4800, epoch 4, loss 8.512961, valid accuracy: 0.918000\n",
      "step 6000, epoch 5, loss 6.699117, valid accuracy: 0.915000\n",
      "step 7200, epoch 6, loss 4.828868, valid accuracy: 0.929000\n",
      "step 8400, epoch 7, loss 3.615759, valid accuracy: 0.927000\n",
      "step 9600, epoch 8, loss 2.540831, valid accuracy: 0.918000\n",
      "step 10800, epoch 9, loss 1.671445, valid accuracy: 0.926000\n",
      "step 12000, epoch 10, loss 0.997087, valid accuracy: 0.920000\n",
      "step 13200, epoch 11, loss 0.665524, valid accuracy: 0.921000\n",
      "step 14400, epoch 12, loss 0.378615, valid accuracy: 0.928000\n",
      "step 15600, epoch 13, loss 0.205762, valid accuracy: 0.929000\n",
      "step 16800, epoch 14, loss 0.135226, valid accuracy: 0.925000\n",
      "step 18000, epoch 15, loss 0.095630, valid accuracy: 0.926000\n",
      "step 19200, epoch 16, loss 0.059717, valid accuracy: 0.925000\n",
      "step 20400, epoch 17, loss 0.044008, valid accuracy: 0.926000\n",
      "step 21600, epoch 18, loss 0.029381, valid accuracy: 0.924000\n",
      "step 22800, epoch 19, loss 0.022045, valid accuracy: 0.922000\n",
      "test accuracy:0.924100\n",
      "training finished,  the model will be save in ./trained_model/\n"
     ]
    }
   ],
   "source": [
    "%run runner.py miniResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
